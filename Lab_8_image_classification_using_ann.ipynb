{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab_8-image_classification_using_ann.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shivani-pawar26/Machine_learning/blob/main/Lab_8_image_classification_using_ann.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT4a4dwlqhsd"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwMdAK1XqmPI"
      },
      "source": [
        "# Tutorial 4: Covid 19 Prediction using Artificial Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9dbM1bxjvnU"
      },
      "source": [
        "Dataset: [Covid 19 Chest X-ray dataset](https://www.kaggle.com/tawsifurrahman/covid19-radiography-database)\n",
        "\n",
        "\n",
        "A team of researchers from Qatar University, Doha, Qatar, and the University of Dhaka, Bangladesh along with their collaborators from Pakistan and Malaysia in collaboration with medical doctors have created a database of chest X-ray images for COVID-19 positive cases along with Normal and Viral Pneumonia images. This COVID-19, normal, and other lung infection dataset is released in stages. In the first release, we have released 219 COVID-19, 1341 normal, and 1345 viral pneumonia chest X-ray (CXR) images. In the first update, we have increased the COVID-19 class to 1200 CXR images. In the 2nd update, we have increased the database to 3616 COVID-19 positive cases along with 10,192 Normal, 6012 Lung Opacity (Non-COVID lung infection), and 1345 Viral Pneumonia images. We will continue to update this database as soon as we have new x-ray images for COVID-19 pneumonia patients.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mHnC6nNtBK1"
      },
      "source": [
        "**1. Mount the Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gORvsC4_s_fY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0420440e-6e67-4d83-f3dd-d782660a8398"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiZOocimtGeu"
      },
      "source": [
        "**2. Move to the place where data resides**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdbGFIkPlah9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38719187-70c4-43b1-adb9-dd5047ba272b"
      },
      "source": [
        "%cd /content/drive/MyDrive/COVID-19_Radiography_Dataset.zip"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 20] Not a directory: '/content/drive/MyDrive/COVID-19_Radiography_Dataset.zip'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqViJFpyjxJZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2af08b1-6eb9-4812-8a5c-bcb157c61d50"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e8v2RY-tKUs"
      },
      "source": [
        "**3. Unziping the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOWDshT6lb8K"
      },
      "source": [
        "# !unzip covid_dataset.zip"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvuRmzA0tNw0"
      },
      "source": [
        "**4. Install split folder python package**\n",
        "\n",
        "https://pypi.org/project/split-folders/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-4b_r2qlenB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e871948-923f-4002-fa9e-2dbb5572015c"
      },
      "source": [
        "!pip install split_folders"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting split_folders\n",
            "  Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: split-folders\n",
            "Successfully installed split-folders-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yq0KuuUatVi-"
      },
      "source": [
        "**5. Splitting the data in training, testing and validation set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xXGHVOIlheA"
      },
      "source": [
        "# import splitfolders\n",
        "# splitfolders.ratio(\"covid_dataset\", output=\"split\", seed=1337, ratio=(.8, .1, .1), group_prefix=None)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_rTBEKCtaTR"
      },
      "source": [
        "**6. Loading the dataset with normalization in batches**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4of-TiQPlkCZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "a196516a-d326-4b75-caad-c84c87fe1f3c"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Normalize training and validation data in the range of 0 to 1\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Read the training sample and set the batch size \n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        'split/train/',\n",
        "        target_size=(128, 128),\n",
        "        batch_size=8,\n",
        "        seed=100,\n",
        "        class_mode='categorical')\n",
        "\n",
        "# Read Validation data from directory and define target size with batch size\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        'split/val/',\n",
        "        target_size=(128, 128),\n",
        "        batch_size=8,\n",
        "        class_mode='categorical',\n",
        "        seed=1000,\n",
        "        shuffle=False)\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        'split/test/',\n",
        "        target_size=(128, 128),\n",
        "        batch_size=8,\n",
        "        seed=500,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-15537ee4718e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         class_mode='categorical')\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Read Validation data from directory and define target size with batch size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation)\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[0mfollow_links\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_links\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m         interpolation=interpolation)\n\u001b[0m\u001b[1;32m    993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m   def flow_from_dataframe(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, dtype)\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m         **kwargs)\n\u001b[0m\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/directory_iterator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, dtype)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'split/train/'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqmlFmyitf8O"
      },
      "source": [
        "**7. Model Building**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrA7f0eKl4tH"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "inputs = keras.Input(shape=(128, 128,3))\n",
        "x = layers.Flatten()(inputs)\n",
        "x = layers.Dense(32, activation=\"relu\")(x)\n",
        "x = layers.Dense(64, activation='relu')(x)\n",
        "outputs = layers.Dense(3, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_437haxhtjFa"
      },
      "source": [
        "**8. Model Compilation and Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DixxjVJelrcg"
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "adam = Adam(learning_rate=0.0001)\n",
        "# We are going to use accuracy metrics and cross entropy loss as performance parameters\n",
        "model.compile(adam, loss='categorical_crossentropy', metrics=['acc'])\n",
        "# Train the model \n",
        "history = model.fit(train_generator, \n",
        "      steps_per_epoch=train_generator.samples/train_generator.batch_size,\n",
        "      epochs=100,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=validation_generator.samples/validation_generator.batch_size,\n",
        "      verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h2kqqa0tpx9"
      },
      "source": [
        "**9. Model saving**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMlZWxBhnfjh"
      },
      "source": [
        "model.save('covid_classification.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v64VrifGtsSn"
      },
      "source": [
        "**10. Model loading**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5kAXwzBoIzp"
      },
      "source": [
        "from tensorflow.keras import models\n",
        "model = models.load_model('covid_classification.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuGiWfjQtvDq"
      },
      "source": [
        "**11. Model weights saving**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brmKRhGtoZVA"
      },
      "source": [
        "model.save_weights('covid_classification_weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_mHfakptxUF"
      },
      "source": [
        "**12. Model weights loading**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oc3YG_vioL9a"
      },
      "source": [
        "model.load_weights('covid_classification_weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJhgz0AAtzcm"
      },
      "source": [
        "**13. Plotting accuracy and loss graph for training and validation dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKLbBshSocU6"
      },
      "source": [
        "train_acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfqOqEpVogvN"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "epochs = range(len(train_acc)) \n",
        "plt.plot(epochs, train_acc, 'b', label='Training Accuracy')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epochs, train_loss, 'b', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoVhONtnt3sq"
      },
      "source": [
        "**14. Evaluate model performance on test dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxXMTffHoiVy"
      },
      "source": [
        "test_output= model.evaluate(test_generator, steps=test_generator.samples/test_generator.batch_size, verbose=1)\n",
        "print(test_output)\n",
        "print(model.metrics_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inzAGua1o1we"
      },
      "source": [
        "References:\n",
        "\n",
        "1. https://pypi.org/project/split-folders/\n",
        "2. https://keras.io/"
      ]
    }
  ]
}